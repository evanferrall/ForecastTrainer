# conf/debug_train_config.yaml
project_name: EscapeRoomDebug
experiment_name: escape_room_gpu_debug_bolt
raw_data_dir: /data/bookings/
escape_room_datamodule:
  csv_path: /home/evan/Dropbox/Pass Through Files/HQ Booking Data (For EBF).csv
  batch_size: 256 # This is datamodule batch_size for PyTorch Lightning, separate from model internal batch_size
  num_workers: 8
  history_cutoff: "2023-01-01" # Lever A: Trim history
model_autogluon:
  target: "y"
  freq: "H"
  prediction_length: 24
  eval_metric: "WAPE"
  time_limit_per_family: 120 # Increased slightly for bolt, can adjust
  fit_hyperparameters:
    Chronos:
      model_path: "autogluon/chronos-bolt-base"  # Lever D: Switch to Chronos-Bolt
      context_length: 168                       # Lever C: Limit context_length
      batch_size: 8                             # Lever E: Lower batch size (Chronos internal)
      fine_tune: true # Keep fine-tuning
      epochs: 10 # Keep epochs, but time_limit_per_family will cap it
      learning_rate: 3e-5
      lr_scheduler_type: "cosine"
      warmup_ratio: 0.05
      lora_r: 4       # Keep reduced LoRA for now
      lora_alpha: 8   # Keep reduced LoRA for now
      trainer_kwargs:
        accelerator: "gpu"
        devices: 1
        precision: "bf16-mixed"
        gradient_checkpointing: true           # Lever F: Enable gradient checkpointing
show_leaderboard: true 