---
description:
globs:
alwaysApply: false
---
# 4. Training loop

Lightning Trainer with

*   gradient accumulation (steps picked so one virtual batch ≈ 32k series)
*   SWA finishing (often °1-2 MAPE on TFT)
*   Early-stopping on validation WAPE (patience = 8).

Optuna study optional (`python [cli.py](mdc:forecast_cli/cli.py) tune`) over:

*   learning-rate log-uniform `[3e-4, 3e-2]`
*   NHITS `n_stacks` `{2,3,4}` & `n_layers` `{1,2,3}`
*   PatchTST `d_model` `{64,128,256}` etc.
*   `λ_hier` in `[0, 5]`.

A ready-made YAML lives in `conf/tune.yaml`.
Training logic can be found in [trainer.py](mdc:forecast_cli/modelling/training/trainer.py) and [tuning.py](mdc:forecast_cli/modelling/training/tuning.py).
