import lightning.pytorch as pl
from lightning.pytorch.callbacks import (EarlyStopping, ModelCheckpoint, 
                                       StochasticWeightAveraging, RichProgressBar)
from lightning.pytorch.loggers import TensorBoardLogger
import os
from typing import List, Callable # For List type hint
import torch # Added for torch.backends.mps.is_available()

DEFAULT_LOGS_DIR = "training_logs"
DEFAULT_CHECKPOINTS_DIR = "model_checkpoints"

def get_trainer(
    experiment_name: str = "default_experiment",
    run_name: str | None = None, # If None, auto-generated by TensorBoardLogger
    max_epochs: int = 100,
    patience_early_stopping: int = 8, # As per roadmap
    accumulate_grad_batches: int = 4, # Placeholder, adjust based on batch size for "virtual batch â‰ˆ 32k series"
    enable_swa: bool = True, # As per roadmap (SWA finishing)
    swa_lrs: float | list[float] = 1e-2, # SWA learning rate, can be tuned
    accelerator: str = "mps" if torch.backends.mps.is_available() else "cpu",
    devices: int = 1,
    logs_base_dir: str = DEFAULT_LOGS_DIR,
    checkpoints_base_dir: str = DEFAULT_CHECKPOINTS_DIR,
    monitor_metric: str = "val/wape_avg", # Monitor average WAPE as discussed
    monitor_mode: str = "min", # Lower WAPE is better
    additional_callbacks: List[Callable] | None = None,
    enable_default_early_stopping: bool = True # New argument
) -> pl.Trainer:
    """
    Configures and returns a PyTorch Lightning Trainer instance based on roadmap specifications.

    Args:
        experiment_name: Name for the experiment (group of runs).
        run_name: Name for this specific run. If None, logger auto-generates.
        max_epochs: Maximum number of epochs to train.
        patience_early_stopping: Patience for EarlyStopping callback.
        accumulate_grad_batches: Number of batches for gradient accumulation.
        enable_swa: Whether to enable Stochastic Weight Averaging.
        swa_lrs: Learning rate(s) for SWA.
        accelerator: Accelerator to use (e.g., "cpu", "gpu", "mps", "auto").
        devices: Devices to use (e.g., number of GPUs or "auto").
        logs_base_dir: Base directory to save logs.
        checkpoints_base_dir: Base directory to save model checkpoints.
        monitor_metric: The metric to monitor for early stopping and model checkpointing.
        monitor_mode: "min" or "max" for the monitored metric.
        additional_callbacks: List of additional callbacks to add to the trainer.
        enable_default_early_stopping: Whether to enable the default EarlyStopping callback.

    Returns:
        A configured PyTorch Lightning Trainer.
    """

    # Logger
    logger = TensorBoardLogger(save_dir=logs_base_dir, name=experiment_name, version=run_name)

    # Callbacks
    callbacks = []

    if enable_default_early_stopping:
        early_stop_callback = EarlyStopping(
            monitor=monitor_metric,
            patience=patience_early_stopping,
            verbose=True,
            mode=monitor_mode,
        )
        callbacks.append(early_stop_callback)

    # Model Checkpoint - Save best model based on validation WAPE
    # Ensure checkpoint directory exists
    checkpoint_dir = os.path.join(checkpoints_base_dir, experiment_name, run_name if run_name else "default_run")
    os.makedirs(checkpoint_dir, exist_ok=True)
    
    model_checkpoint_callback = ModelCheckpoint(
        dirpath=checkpoint_dir,
        filename=f"{{epoch}}-{{{monitor_metric}:.4f}}",
        monitor=monitor_metric,
        mode=monitor_mode,
        save_top_k=1,
        save_last=True, # Save the last model as well
        verbose=True,
    )
    callbacks.append(model_checkpoint_callback)

    # Stochastic Weight Averaging (SWA) - Roadmap: SWA finishing
    if enable_swa:
        swa_callback = StochasticWeightAveraging(swa_lrs=swa_lrs)
        callbacks.append(swa_callback)
    
    # Progress Bar
    callbacks.append(RichProgressBar())

    # Add any additional callbacks passed by the user
    if additional_callbacks:
        callbacks.extend(additional_callbacks)

    # Trainer initialization
    trainer = pl.Trainer(
        max_epochs=max_epochs,
        logger=logger,
        callbacks=callbacks,
        accumulate_grad_batches=accumulate_grad_batches,
        accelerator=accelerator,
        devices=devices,
        precision="32-true", # Added precision for M1 MPS
        gradient_clip_val=0.5, # Add gradient clipping
        # deterministic=True, # For reproducibility, consider making this configurable
        # precision='16-mixed' # For mixed-precision training, if needed
    )

    return trainer

if __name__ == "__main__":
    # Example usage (primarily for testing this script, actual training would be elsewhere)
    print("Configuring a default trainer...")
    # Create dummy model and dataloaders for basic trainer instantiation test
    class DummyModel(pl.LightningModule):
        def __init__(self):
            super().__init__()
            self.linear = torch.nn.Linear(10,1)
        def forward(self, x): return self.linear(x)
        def training_step(self, batch, batch_idx):
            x,y = batch
            loss = torch.nn.functional.mse_loss(self(x),y)
            self.log("train/loss", loss)
            return loss
        def validation_step(self, batch, batch_idx):
            x,y = batch
            loss = torch.nn.functional.mse_loss(self(x),y)
            self.log("val/loss_total", loss)
            # Log dummy wape for callback testing
            self.log("val/wape_avg", torch.rand(1))
        def configure_optimizers(self):
            return torch.optim.SGD(self.parameters(), lr=0.01)

    import torch
    from torch.utils.data import DataLoader, TensorDataset
    dummy_model = DummyModel()
    train_data = TensorDataset(torch.randn(100,10), torch.randn(100,1))
    val_data = TensorDataset(torch.randn(50,10), torch.randn(50,1))
    train_loader = DataLoader(train_data, batch_size=10)
    val_loader = DataLoader(val_data, batch_size=10)

    trainer_instance = get_trainer(
        experiment_name="test_experiment", 
        run_name="test_run", 
        max_epochs=3
    )
    print(f"Trainer configured. Log directory: {trainer_instance.logger.log_dir}")
    print(f"To run a dummy training: trainer_instance.fit(dummy_model, train_dataloaders=train_loader, val_dataloaders=val_loader)")
    # try:
    #     trainer_instance.fit(dummy_model, train_dataloaders=train_loader, val_dataloaders=val_loader)
    #     print("Dummy training finished.")
    # except Exception as e:
    #     print(f"Error during dummy training: {e}")
